#!/usr/bin/env python3
"""
Quick Gemini CLI - Chat with Gemini from terminal
Usage:
  gemini "Your question here"
  gemini -m flash "Fast response"
  gemini -m pro "More capable response"
  gemini -t 0.9 "Creative response"
  gemini -c "Continue conversation"
"""

import os
import sys
import json
import urllib.request
import urllib.error
from pathlib import Path

# Configuration - SECURITY FIX: No hardcoded API key
API_KEY = os.environ.get('GOOGLE_API_KEY')
if not API_KEY:
    print("❌ Error: GOOGLE_API_KEY environment variable not set", file=sys.stderr)
    print("Set your API key: export GOOGLE_API_KEY='your-key-here'", file=sys.stderr)
    sys.exit(1)

HISTORY_FILE = Path.home() / '.gemini_history.json'

MODELS = {
    'flash': 'gemini-2.0-flash',
    'lite': 'gemini-2.0-flash-lite',
    'pro': 'gemini-2.5-pro',
    '2.5': 'gemini-2.5-flash'
}

def load_history():
    """Load conversation history"""
    if HISTORY_FILE.exists():
        with open(HISTORY_FILE) as f:
            return json.load(f)
    return []

def save_history(history):
    """Save conversation history with secure file permissions"""
    # SECURITY FIX: Create file with restrictive permissions (600 = owner read/write only)
    import stat

    # Write history securely
    with open(HISTORY_FILE, 'w') as f:
        json.dump(history[-10:], f)

    # Set restrictive permissions: owner read/write only
    os.chmod(HISTORY_FILE, stat.S_IRUSR | stat.S_IWUSR)  # 0o600

def sanitize_prompt(prompt):
    """SECURITY FIX: Basic input validation"""
    # Limit prompt length to prevent abuse
    MAX_PROMPT_LENGTH = 10000
    if len(prompt) > MAX_PROMPT_LENGTH:
        raise ValueError(f"Prompt too long (max {MAX_PROMPT_LENGTH} characters)")

    # Ensure prompt is string
    if not isinstance(prompt, str):
        raise TypeError("Prompt must be a string")

    return prompt.strip()

def call_gemini(prompt, model='flash', temperature=0.7, continue_chat=False):
    """Call Gemini API with input sanitization"""
    # SECURITY FIX: Sanitize user input
    prompt = sanitize_prompt(prompt)

    model_name = MODELS.get(model, model)
    url = f'https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={API_KEY}'

    # Handle conversation history
    if continue_chat:
        history = load_history()
        history.append({'role': 'user', 'parts': [{'text': prompt}]})
        contents = history
    else:
        contents = [{'role': 'user', 'parts': [{'text': prompt}]}]
        history = []

    data = {
        'contents': contents,
        'generationConfig': {
            'temperature': temperature,
            'maxOutputTokens': 1000
        }
    }

    try:
        req = urllib.request.Request(
            url,
            data=json.dumps(data).encode('utf-8'),
            headers={'Content-Type': 'application/json'}
        )

        response = urllib.request.urlopen(req, timeout=30)
        result = json.loads(response.read().decode('utf-8'))

        if 'candidates' in result and result['candidates']:
            text = result['candidates'][0]['content']['parts'][0]['text']

            # Update history
            if continue_chat:
                history.append(result['candidates'][0]['content'])
                save_history(history)
            else:
                save_history([
                    {'role': 'user', 'parts': [{'text': prompt}]},
                    result['candidates'][0]['content']
                ])

            return text
        else:
            return "No response generated"

    except urllib.error.HTTPError as e:
        error_body = json.loads(e.read().decode('utf-8'))
        return f"Error: {error_body['error']['message']}"
    except Exception as e:
        return f"Error: {e}"

def interactive_mode():
    """Interactive mode with custom prompt and personality selection"""

    # Monolith ASCII branding
    print()
    print("╔════════════════════════════════════════════════════╗")
    print("║  🗿 MONOLITH GEMINI VALIDATOR                      ║")
    print("║     Creator/Validator Architecture                ║")
    print("╚════════════════════════════════════════════════════╝")
    print()
    print("🤖 Models (switch anytime):")
    print("  flash, pro, lite, 2.5")
    print()
    print("🎭 16-Persona Validation:")
    print()
    print("  Core:        fact, code, logic, ambiguity, optimize")
    print("  Analysis:    practical, critical, strategic, security")
    print("  Perspective: educational, philosophical, economic, social")
    print("  Research:    historical, scientific, sonnet")
    print()
    print("📋 Commands:")
    print()
    print("  <persona> <content>         - Validate with persona (e.g., fact <claim>)")
    print("  all <content>               - Run ALL 16 personas on content")
    print("  persona <name>              - Show exact prompt for a persona")
    print("  guide                       - Show full guide")
    print("  clear                       - Clear conversation history")
    print("  exit                        - Exit to terminal")
    print()
    print("💡 Tip: Just type your message directly (no command needed)")
    print("─" * 54)
    print()

    current_model = 'flash'

    while True:
        try:
            # Custom prompt showing current personality
            user_input = input(f"🗿 gemini ({current_model}) ❯ ").strip()

            if not user_input:
                continue

            # Parse commands (no prefix needed)
            parts = user_input.split(maxsplit=1)
            cmd = parts[0].lower()

            if cmd == 'exit' or cmd == 'quit':
                print()
                print("👋 Exiting Gemini Validator")
                print()
                break

            elif cmd == 'guide' or cmd == 'help':
                # Show the full guide
                print()
                print("╔════════════════════════════════════════════════════╗")
                print("║  📖 GEMINI VALIDATOR GUIDE                         ║")
                print("╚════════════════════════════════════════════════════╝")
                print()
                print("🤖 Models (switch anytime):")
                print("  flash, pro, lite, 2.5")
                print()
                print("🎭 16-Persona Validation:")
                print()
                print("  Core:        fact, code, logic, ambiguity, optimize")
                print("  Analysis:    practical, critical, strategic, security")
                print("  Perspective: educational, philosophical, economic, social")
                print("  Research:    historical, scientific, sonnet")
                print()
                print("📋 Commands:")
                print()
                print("  <persona> <content>         - Validate with persona (e.g., fact <claim>)")
                print("  all <content>               - Run ALL 16 personas on content")
                print("  persona <name>              - Show exact prompt for a persona")
                print("  guide                       - Show this guide")
                print("  clear                       - Clear conversation history")
                print("  exit                        - Exit to terminal")
                print()
                print("💡 Tip: Just type your message directly (no command needed)")
                print("─" * 54)
                print()
                continue

            elif cmd == 'clear':
                if HISTORY_FILE.exists():
                    HISTORY_FILE.unlink()
                    print("✅ Conversation history cleared")
                continue

            elif cmd == 'persona':
                if len(parts) < 2:
                    print("Usage: persona <name>")
                    print("Available: fact, code, logic, ambiguity, optimize,")
                    print("           practical, critical, strategic, security,")
                    print("           educational, philosophical, economic, social,")
                    print("           historical, scientific, sonnet")
                    continue

                # Show the exact prompt from gemini-validate
                import subprocess
                result = subprocess.run(
                    ['grep', '-A', '20', f"'{parts[1]}':", '/home/thommy/projects/gemini-validator/gemini-validate'],
                    capture_output=True,
                    text=True
                )

                if result.returncode == 0 and result.stdout:
                    print()
                    print(f"╔════════════════════════════════════════════════════╗")
                    print(f"║  📋 PERSONA: {parts[1].upper():<38}║")
                    print(f"╚════════════════════════════════════════════════════╝")
                    print()
                    # Extract just the prompt text
                    lines = result.stdout.split('\n')
                    in_prompt = False
                    prompt_lines = []
                    for line in lines:
                        if '"""' in line:
                            if in_prompt:
                                break
                            in_prompt = True
                            continue
                        if in_prompt:
                            prompt_lines.append(line)

                    print('\n'.join(prompt_lines))
                    print()
                    print("─" * 54)
                    print(f"💡 Edit in: /home/thommy/projects/gemini-validator/gemini-validate")
                    print()
                else:
                    print(f"❌ Persona '{parts[1]}' not found")
                continue

            # All 16 personas
            elif cmd in ['fact', 'code', 'logic', 'ambiguity', 'optimize',
                        'practical', 'critical', 'strategic', 'security',
                        'educational', 'philosophical', 'economic', 'social',
                        'historical', 'scientific', 'sonnet']:
                if len(parts) < 2:
                    print(f"Usage: {cmd} <content>")
                    continue
                # Quick validation
                print()
                print(f"🔍 Validating ({cmd})...")
                import subprocess
                result = subprocess.run(
                    ['gemini-validate', cmd, parts[1]],
                    capture_output=True,
                    text=True
                )
                print(result.stdout)
                continue

            elif cmd == 'all':
                # Run all 16 personas
                if len(parts) < 2:
                    print("Usage: all <content to validate>")
                    continue

                personas = ['practical', 'critical', 'strategic', 'security',
                           'educational', 'philosophical', 'economic', 'social',
                           'historical', 'scientific', 'fact', 'code', 'logic',
                           'ambiguity', 'optimize', 'sonnet']

                print()
                print("╔════════════════════════════════════════════════════╗")
                print("║  🎭 16-PERSONA COMPREHENSIVE ANALYSIS              ║")
                print("╚════════════════════════════════════════════════════╝")
                print()

                import subprocess
                for i, persona in enumerate(personas, 1):
                    print(f"[{i}/16] {persona.upper()}...")
                    result = subprocess.run(
                        ['gemini-validate', persona, parts[1]],
                        capture_output=True,
                        text=True
                    )
                    print(result.stdout)
                    print()

                print("─" * 54)
                print("✅ 16-Persona Analysis Complete")
                print()
                continue

            elif cmd in ['flash', 'pro', 'lite', '2.5']:
                current_model = cmd
                print(f"✅ Switched to {current_model.upper()} model")
                continue

            else:
                # Treat as direct chat message (but warn about limitations)
                prompt = user_input

                # Warn if asking about current events
                if any(keyword in user_input.lower() for keyword in ['today', 'score', 'news', 'latest', 'current', 'now', 'yesterday']):
                    print()
                    print("⚠️  Warning: Gemini doesn't have real-time data")
                    print("   Use web search for current events/scores")
                    print()

            # Call Gemini with current model
            print()
            print("╔════════════════════════════════════════════════════╗")
            print(f"║  🤖 {current_model.upper():<46}║")
            print("╚════════════════════════════════════════════════════╝")
            print()

            response = call_gemini(prompt, current_model, 0.7, False)
            print(response)
            print()
            print("─" * 54)
            print()

        except KeyboardInterrupt:
            print()
            print()
            print("👋 Exiting Gemini Validator (Ctrl+C)")
            print()
            break
        except EOFError:
            print()
            print()
            print("👋 Exiting Gemini Validator (EOF)")
            print()
            break

def main():
    import argparse

    parser = argparse.ArgumentParser(description='Chat with Gemini')
    parser.add_argument('prompt', nargs='*', help='Your prompt')
    parser.add_argument('-m', '--model', default='flash', choices=list(MODELS.keys()),
                       help='Model to use (default: flash)')
    parser.add_argument('-t', '--temperature', type=float, default=0.7,
                       help='Temperature (0.0-1.0, default: 0.7)')
    parser.add_argument('-c', '--continue', dest='continue_chat', action='store_true',
                       help='Continue previous conversation')
    parser.add_argument('--clear', action='store_true',
                       help='Clear conversation history')

    args = parser.parse_args()

    if args.clear:
        if HISTORY_FILE.exists():
            HISTORY_FILE.unlink()
            print("✅ Conversation history cleared")
        return

    if not args.prompt:
        # Start interactive mode
        interactive_mode()
        return

    prompt = ' '.join(args.prompt)

    # ASCII header
    print()
    print("╔════════════════════════════════════════════════════╗")
    print(f"║  🤖 Gemini {args.model.upper():<20}", end="")
    if args.continue_chat:
        print("(continuing)        ║")
    else:
        print("                    ║")
    print("╚════════════════════════════════════════════════════╝")
    print()

    response = call_gemini(prompt, args.model, args.temperature, args.continue_chat)
    print(response)
    print()

    if 'Error' not in response:
        print("─" * 54)
        print("💡 Use 'gemini -c \"follow up\"' to continue this chat")
        print()

if __name__ == '__main__':
    main()